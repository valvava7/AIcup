{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0676fd47-cb25-4961-8321-a93ab2c419ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random \n",
    "import time \n",
    "import shutil\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.io import read_image, ImageReadMode, write_png\n",
    "import torchvision.transforms.v2 as v2\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import cv2\n",
    "import py_sod_metrics\n",
    "\n",
    "from model import *\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fce6a4c-01a2-4918-ad49-5747199ca6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seeding\n",
    "##seeding is not implemented for now, sorry.\n",
    "seedeverything(int(time.time()))\n",
    "#seting\n",
    "device = torch.device(\"cuda:4\")\n",
    "VAL_FRAC = 0.15\n",
    "IMG_SIZE = (240,240)    # use (240, 428) for best perf. unless your are debugging\n",
    "TRAINING_PATH = './Training_dataset/'\n",
    "TESTING_PATH = './Testing_dataset/'\n",
    "PRIVATE_PATH = './Private_dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3e5756-bcaf-4d8c-a9ab-0f9e0e81dada",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model\n",
    "max_epoch = 35\n",
    "lr = 0.001\n",
    "model = UNet(IMG_SIZE)\n",
    "optim = Adam(model.parameters(), lr = lr, weight_decay =0.0)#0.00001\n",
    "scheduler = StepLR(optim, step_size=15, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63826c6f-2c28-4e52-9426-8f06731a5b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "##split training and validation data\n",
    "names = sorted(f[:-4] for f in os.listdir( TRAINING_PATH + 'img') if f.endswith('jpg'))\n",
    "divider = int(len(names)*VAL_FRAC)\n",
    "random.shuffle(names)\n",
    "val_names, training_names = names[:divider], names[divider:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8023e60-1014-4048-8fec-a771aad774a8",
   "metadata": {},
   "source": [
    "## **define dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17dacf6-721a-4200-9a47-15eb36dc2003",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, names,augmentation = False, preload = True, to_gpu = False):\n",
    "        '''\n",
    "        args:\n",
    "            augmentation:if False, consider this is a validation dataset.(otherwise a training dataset)\n",
    "            preload: preload all img(label) into a tensor, reduce abt 20% time of data loading\n",
    "            to_gpu: If True, transfer all preload data to ${device}. Significntly reduce time of data loading and transfering. \n",
    "                    note that you need enough gpu memory or may get an error.\n",
    "        '''\n",
    "        self.names = names\n",
    "        self.aug = augmentation\n",
    "        self.preload = preload\n",
    "        self.to_gpu = to_gpu\n",
    "        self.img_transform = v2.Compose([\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "            v2.Resize(IMG_SIZE, antialias=True),\n",
    "            Clip(),\n",
    "            v2.Normalize((0.519,0.535,0.442),(0.196, 0.175, 0.207)),  ##(0.519,0.535,0.442),(0.196, 0.175, 0.207) or (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)\n",
    "        ])\n",
    "        \n",
    "        self.label_transform = v2.Compose([\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "            v2.Resize(IMG_SIZE, antialias=True),\n",
    "            Clip(),\n",
    "            #Binarize(0.5), # use this if using Dice_Loss\n",
    "        ])\n",
    "        \n",
    "        if self.preload:\n",
    "            self.img_buffer = [read_image(TRAINING_PATH + 'img/' + name + '.jpg', mode = ImageReadMode.RGB) for name in names]\n",
    "            self.label_buffer = [read_image(TRAINING_PATH + 'label_img/' + name+'.png', mode = ImageReadMode.GRAY) for name in names]\n",
    "            self.img_buffer = torch.stack(self.img_buffer, dim=0)\n",
    "            self.label_buffer = torch.stack(self.label_buffer, dim=0)\n",
    "            if self.to_gpu:\n",
    "                self.img_buffer, self.label_buffer = self.img_buffer.to(device), self.label_buffer.to(device)\n",
    "            self.img_buffer = self.img_transform(self.img_buffer)\n",
    "            self.label_buffer = self.label_transform(self.label_buffer)\n",
    "\n",
    "        \n",
    "        # apply flip, rotate...here\n",
    "        self.aug_transform = v2.Compose([\n",
    "            v2.RandomHorizontalFlip(),\n",
    "        ])\n",
    "        self.img_transform_1 = v2.Compose([\n",
    "            #v2.RandomApply(torch.nn.ModuleList([]), p=0.2),\n",
    "            #RandomChannelSwap(0.1),\n",
    "            #v2.RandomGrayscale(0.1),\n",
    "            add_gaussian_noise(0.05),\n",
    "            #v2.GaussianBlur(5),\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "    @torch.no_grad()\n",
    "    def __getitem__(self, index):\n",
    "        name = self.names[index]\n",
    "        #preprocess pipeline\n",
    "        #img:img_transform -> img_transform_1 -> (aug_transform)\n",
    "        #label:label_transform -> aug_tramsform\n",
    "        if self.preload:\n",
    "            img = self.img_buffer[index].clone()\n",
    "            label = self.label_buffer[index].clone()\n",
    "        else:\n",
    "            img = read_image(TRAINING_PATH + 'img/' + name + '.jpg', mode = ImageReadMode.RGB)\n",
    "            label = read_image(TRAINING_PATH + 'label_img/' + name+'.png', mode = ImageReadMode.GRAY)\n",
    "            img = self.img_transform(img)\n",
    "            label = self.label_transform(label)\n",
    "        if self.aug:\n",
    "            img = self.img_transform_1(img)\n",
    "            rng = torch.random.get_rng_state()\n",
    "            img = self.aug_transform(img)\n",
    "            torch.random.set_rng_state(rng)\n",
    "            label = self.aug_transform(label)\n",
    "        return img, label\n",
    "    '''\n",
    "    def _try_rng(self): # try if augmentation is ok\n",
    "        for i in range(10):\n",
    "            img = read_image(TRAINING_PATH + 'img/' + random.choice(self.names) + '.jpg', mode = ImageReadMode.RGB)\n",
    "            rng = torch.random.get_rng_state()\n",
    "            tran1 = self.aug_transform(img)\n",
    "            torch.random.set_rng_state(rng)\n",
    "            tran2 = self.aug_transform(img)\n",
    "            if not (tran1==tran2).all().item():\n",
    "                print('something went wrong!!')\n",
    "                return\n",
    "        print('augmentation checked.')\n",
    "        return\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2050737-defc-4c40-9b57-f6eef7dfaf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare dataset\n",
    "start_time = time.time()\n",
    "train_dataset = ImageDataset(training_names, augmentation = True, to_gpu = True) # take abt 2.5 min\n",
    "print(time.time()-start_time)\n",
    "start_time = time.time()\n",
    "val_dataset = ImageDataset(val_names, augmentation = False, to_gpu = True) # take abt 0.5 min\n",
    "print(time.time()-start_time)\n",
    "\n",
    "#train_dataset._try_rng()\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(dataset = val_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24ce82c-bbac-459b-8d55-4449d8083840",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7f305a-6f8f-46a8-a2ab-0e89a495f311",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#training\n",
    "model.to(device)\n",
    "for epoch in range(max_epoch):\n",
    "    ## training\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loss_cnt = 0\n",
    "    #####timing#####\n",
    "    load_time, trans_time, comp_time = 0,0,0\n",
    "    start_time = time.time()\n",
    "    ################\n",
    "    for x, y in tqdm(train_loader, leave = False, desc = 'training'):\n",
    "        #####timing#####\n",
    "        mid_time = time.time()\n",
    "        ################\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        #####timing#####\n",
    "        end_time = time.time()\n",
    "        ################\n",
    "        y_hat = model(x)\n",
    "        loss = F.binary_cross_entropy(y_hat, y)#(F.binary_cross_entropy(y_hat, y) + dl_criterion(y_hat, y))/2\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        total_loss += loss * len(x)\n",
    "        loss_cnt += len(x)\n",
    "\n",
    "\n",
    "        #####timing#####\n",
    "        comp_time += time.time()-end_time\n",
    "        trans_time += end_time-mid_time\n",
    "        load_time += mid_time-start_time\n",
    "        start_time = time.time()\n",
    "        ################\n",
    "    #print(f'loading time:{load_time:.1f}s, transfering time:{trans_time:.1f}s, computing time:{comp_time:.1f}s')\n",
    "\n",
    "    \n",
    "    train_loss = total_loss/loss_cnt\n",
    "    ## validation\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    loss_cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(val_loader, leave = False, desc = 'validating'):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_hat = model(x)\n",
    "            loss = F.binary_cross_entropy(y_hat, y)\n",
    "            total_loss += loss * len(x)\n",
    "            loss_cnt += len(x)\n",
    "        val_loss = total_loss/loss_cnt\n",
    "    print(f'epoch{epoch+1:3d}: train loss {train_loss:.4f}\\tval loss {val_loss:.4f}','\\t|\\t'\n",
    "         f'cost: ld {load_time:.1f}s, trans {trans_time:.1f}s, comp {comp_time:.1f}s')\n",
    "\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608b5f7e-59a0-48d2-9f95-7320ca5030ea",
   "metadata": {},
   "source": [
    "# **validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c674ade1-5b12-482e-b4f2-29138867668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(folder, names, model, transform, output_folder, binarize = True, device = 'cuda:0'):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    if names is None:\n",
    "        names = sorted(f[:-4] for f in os.listdir(folder) if f.endswith('jpg'))\n",
    "    saved_cnt = 0\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for name in tqdm(names, leave = False, desc = 'inferencing'):\n",
    "            img = read_image(folder+name+'.jpg', mode = ImageReadMode.RGB)\n",
    "            img = transform(img).unsqueeze(0).to(device)\n",
    "            predict = nn.functional.interpolate(model(img), size=(240, 428), mode='bilinear', align_corners=False)\n",
    "            if binarize:\n",
    "                predict = (predict >0.5)* 255\n",
    "            else:\n",
    "                predict = predict * 255\n",
    "            predict = predict.squeeze(0).to('cpu', dtype = torch.uint8 )\n",
    "            write_png(predict, output_folder+name+'.png')\n",
    "            saved_cnt += 1\n",
    "    print(f'{saved_cnt} files saved to {output_folder}')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1c97f1-79df-421d-862a-95cc5a9efc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use validation set to evaluate\n",
    "inference(TRAINING_PATH+'img/', val_names, model, train_dataset.img_transform, './tmp/', binarize = True, device = device)\n",
    "\n",
    "\n",
    "FMv2 = py_sod_metrics.FmeasureV2(\n",
    "    metric_handlers={\n",
    "        \"fm\": py_sod_metrics.FmeasureHandler(with_dynamic=True, with_adaptive=False, beta=0.3),\n",
    "    }\n",
    ")\n",
    "\n",
    "for name in val_names:\n",
    "    label = cv2.imread(TRAINING_PATH+'label_img/'+name+'.png', cv2.IMREAD_GRAYSCALE)\n",
    "    predict = cv2.imread('./tmp/'+name+'.png', cv2.IMREAD_GRAYSCALE)\n",
    "    FMv2.step(pred=predict, gt=label)\n",
    "\n",
    "fmv2 = FMv2.get_results()\n",
    "print(\"mean F score: \",fmv2[\"fm\"][\"dynamic\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a38249-9c22-4744-9771-37cabb86bf57",
   "metadata": {},
   "source": [
    "# **submiting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5173a206-492c-4aa1-95af-8a834a55ca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output public testing folder\n",
    "\n",
    "# if you want to load model, use next two line. Note that you should use the weight save by torchscript\n",
    "MODEL_PATH = './weights/0528-2257.pt' \n",
    "model = torch.jit.load(MODEL_PATH)\n",
    "#print(f'load {MODEL_PATH}')\n",
    "\n",
    "dummy_dataset = ImageDataset([], preload=False) \n",
    "\n",
    "t = time.localtime()\n",
    "t = f'{t.tm_mon:02d}{t.tm_mday:02d}-{t.tm_hour:02d}{t.tm_min:02d}'\n",
    "print('submit-'+t)\n",
    "output_folder = './submit/submit-'+t+'/'\n",
    "inference(TESTING_PATH, None, model, dummy_dataset.img_transform, output_folder, binarize = True, device = device)\n",
    "inference(PRIVATE_PATH, None, model, dummy_dataset.img_transform, output_folder, binarize = True, device = device)\n",
    "shutil.make_archive('submit/submit-'+t, 'zip', output_folder)\n",
    "\n",
    "torch.save(model.state_dict(), './weights/'+t+'_.pt')\n",
    "torch.jit.script(model).save('./weights/'+t+'.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf7d6c1-d7e5-45af-9fc7-43269da04e81",
   "metadata": {},
   "source": [
    "# **calculate std and mean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8f461a-04f2-4a8a-9e91-351a4f8aae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc0, tc1, tc2 = [],[],[]\n",
    "vc0, vc1, vc2 = [],[],[]\n",
    "\n",
    "for i in range(len(train_dataset)):\n",
    "    tc0.append(train_dataset[i][0][0,:,:])\n",
    "    tc1.append(train_dataset[i][0][1,:,:])\n",
    "    tc2.append(train_dataset[i][0][2,:,:])\n",
    "    \n",
    "for i in range(len(val_dataset)):\n",
    "    vc0.append(val_dataset[i][0][0,:,:])\n",
    "    vc1.append(val_dataset[i][0][1,:,:])\n",
    "    vc2.append(val_dataset[i][0][2,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c022b388-3c03-490c-9658-5537cba443ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttc0 = torch.stack(tc0, dim=0)\n",
    "ttc1 = torch.stack(tc1, dim=0)\n",
    "ttc2 = torch.stack(tc2, dim=0)\n",
    "tvc0 = torch.stack(vc0, dim=0)\n",
    "tvc1 = torch.stack(vc1, dim=0)\n",
    "tvc2 = torch.stack(vc2, dim=0)\n",
    "print('----std_mean in training set----')\n",
    "print(torch.std_mean(ttc0))\n",
    "print(torch.std_mean(ttc1))\n",
    "print(torch.std_mean(ttc2))\n",
    "print('----std_mean in validation set----')\n",
    "print(torch.std_mean(tvc0))\n",
    "print(torch.std_mean(tvc1))\n",
    "print(torch.std_mean(tvc2))\n",
    "print('----std_mean in all dataset(training + validating)----')\n",
    "print(torch.std_mean(torch.cat( (ttc0,tvc0) )))\n",
    "print(torch.std_mean(torch.cat( (ttc1,tvc1) )))\n",
    "print(torch.std_mean(torch.cat( (ttc2,tvc2) )))\n",
    "#->[(0.519,0.535,0.442),(0.196, 0.175, 0.207)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

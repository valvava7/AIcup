{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aecc3d31-2eeb-4407-8a4a-b532fa98cddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first part:\n",
    "#a test for how many score can a \"perfect model\" get after picture was resized\n",
    "#image resize to IMG_SIZE ,then resize back to (240,428)\n",
    "#compute fm-score using original picture and resized picture\n",
    "\n",
    "#second part:\n",
    "#noticed that py_sod_metric do normalization on your prediction by default,\n",
    "#so i wanna test whether the same thing was done on leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25a858b9-8287-4480-917c-ce1bfd82c627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from torchvision.io import read_image, ImageReadMode, write_png\n",
    "import torchvision.transforms.v2 as v2\n",
    "import torch.nn.functional as F\n",
    "import py_sod_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758f4c53-62ff-43da-9d0e-2d86b6bc93fd",
   "metadata": {},
   "source": [
    "# **about model output size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f39cd5c5-3ef6-45e2-af9f-a51b07983707",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#e = VAEencoder_res(10)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#d = VAEdecoder_res(10)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#model = VAE(decoder=d, encoder=e)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m240\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m240\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mscript(model)\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweights/test.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchsummary/torchsummary.py:60\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     57\u001b[0m     input_size \u001b[38;5;241m=\u001b[39m [input_size]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# batch_size of 2 for batchnorm\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m x \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m*\u001b[39min_size)\u001b[38;5;241m.\u001b[39mtype(dtype) \u001b[38;5;28;01mfor\u001b[39;00m in_size \u001b[38;5;129;01min\u001b[39;00m input_size]\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# print(type(x[0]))\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# create properties\u001b[39;00m\n\u001b[1;32m     64\u001b[0m summary \u001b[38;5;241m=\u001b[39m OrderedDict()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchsummary/torchsummary.py:60\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     57\u001b[0m     input_size \u001b[38;5;241m=\u001b[39m [input_size]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# batch_size of 2 for batchnorm\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m x \u001b[38;5;241m=\u001b[39m [\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43min_size\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m in_size \u001b[38;5;129;01min\u001b[39;00m input_size]\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# print(type(x[0]))\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# create properties\u001b[39;00m\n\u001b[1;32m     64\u001b[0m summary \u001b[38;5;241m=\u001b[39m OrderedDict()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from model import *\n",
    "from torchsummary import summary\n",
    "model = UNet((240,240))\n",
    "#e = VAEencoder_res(10)\n",
    "#d = VAEdecoder_res(10)\n",
    "#model = VAE(decoder=d, encoder=e)\n",
    "model.to(\"cuda:4\")\n",
    "summary(model, (3,240,240))\n",
    "torch.jit.script(model).save('weights/test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e64f39d-05c2-4d43-ab0c-4f107d5e4b3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'UNet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m models \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     models\u001b[38;5;241m.\u001b[39mappend(\u001b[43mUNet\u001b[49m((\u001b[38;5;241m240\u001b[39m,\u001b[38;5;241m240\u001b[39m)))\n\u001b[1;32m      4\u001b[0m     models[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:7\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _ \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'UNet' is not defined"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for _ in range(1000):\n",
    "    models.append(UNet((240,240)))\n",
    "    models[-1].to('cuda:7')\n",
    "    if _ % 20 ==0:\n",
    "        print(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fd779e-aa83-43f1-992e-b5a850eb9ce4",
   "metadata": {},
   "source": [
    "# **about resize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "976ddd8a-f511-4ada-bc5b-fec7ad84d7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './Training_dataset/label_img/TRA_RI_2000000.png'\n",
    "IMG_SIZE = (240, 240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04773549-a73e-4506-952f-f23dd516ad96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(pred, gt):\n",
    "    FMv2 = py_sod_metrics.FmeasureV2(\n",
    "        metric_handlers={\n",
    "            \"fm\": py_sod_metrics.FmeasureHandler(with_dynamic=True, with_adaptive=False, beta=0.3),\n",
    "        }\n",
    "    )\n",
    "    FMv2.step(pred=pred, gt=gt)\n",
    "    fmv2 = FMv2.get_results()\n",
    "    print(\"mean F score: \",fmv2[\"fm\"][\"dynamic\"].mean())\n",
    "    return fmv2[\"fm\"][\"dynamic\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df5e83bf-989b-480a-9c98-634d9fc117a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using v2.Resize(antialias=True) for second resize\n",
      "mean F score:  0.9957230709909372\n",
      "----\n",
      "using F.interpolate(mode='bilinear', align_corners=False) for second resize\n",
      "mean F score:  0.9957230709909372\n",
      "----\n",
      "only resize once((240,428)->(240,428)) using F.interpolate(mode=\"bilinear\", align_corners=False)\n",
      "mean F score:  0.996176789561585\n",
      "----\n",
      "score between img itself\n",
      "mean F score:  0.996176789561585\n"
     ]
    }
   ],
   "source": [
    "def test(img_path):\n",
    "    img = read_image(img_path, mode = ImageReadMode.GRAY)\n",
    "    #img = ((img>128)*255).float()\n",
    "    img1_1 = v2.Resize(IMG_SIZE, antialias = True)(img)\n",
    "    img1_1 = v2.ToDtype(torch.float32, scale = True)(img1_1)\n",
    "    #img1_1 = ((img1_1>0.5)*255.0)\n",
    "    img1_2 = v2.Resize((240,428), antialias = True)(img1_1)\n",
    "    img1_3 = F.interpolate(img1_1.unsqueeze(0), size=(240, 428), mode='bilinear', align_corners=False).squeeze(0)\n",
    "    img1_4 = F.interpolate(img.unsqueeze(0), size=(240, 428), mode='bilinear', align_corners=False).squeeze(0)\n",
    "\n",
    "    img1_2 = (img1_2>0.5)*255\n",
    "    img1_3 = (img1_3>0.5)*255\n",
    "    img2_0 = img.squeeze(0).numpy()\n",
    "    img2_2 = img1_2.squeeze(0).numpy()\n",
    "    img2_3 = img1_3.squeeze(0).numpy()\n",
    "    img2_4 = img1_4.squeeze(0).numpy()\n",
    "\n",
    "\n",
    "    print('using v2.Resize(antialias=True) for second resize')\n",
    "    score(img2_2,img2_0)\n",
    "    print('----')\n",
    "    print(\"using F.interpolate(mode='bilinear', align_corners=False) for second resize\")\n",
    "    score(img2_3,img2_0)\n",
    "\n",
    "    #img3_2 = img1_2.numpy()\n",
    "    #print((img3_2==img2_2).all())\n",
    "    #print(img.squeeze(0).numpy().shape, img2_2.shape)\n",
    "\n",
    "    print('----')\n",
    "    print('only resize once((240,428)->(240,428)) using F.interpolate(mode=\"bilinear\", align_corners=False)')\n",
    "    score(img2_4, img2_0)\n",
    "\n",
    "    print('----')\n",
    "    print('score between img itself')\n",
    "    score(img.squeeze(0).numpy(),img.squeeze(0).numpy())\n",
    "test(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68f2a489-e1a3-4108-a289-c49900567275",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './Training_dataset/img/TRA_RI_2000000.jpg'\n",
    "img = read_image(path, mode = ImageReadMode.RGB)\n",
    "img = v2.Grayscale()(img)\n",
    "write_png(img, './tmp/test.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e4289f-4808-446e-abb4-cafb1459ad20",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **about normalization before scoring(in server end)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5166e38-9fa1-440a-a062-c369c6a667a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#names = os.listdir('./submit/submit-0514-1338/')\n",
    "#for name in names:\n",
    "#    img = read_image('./submit/submit-0514-1338/'+name, mode = ImageReadMode.GRAY)\n",
    "#    img_ = img//2\n",
    "#    write_png(img_, './submit/submit-0514-1338/'+name)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad761248-6a32-4ff8-a577-127df04ffdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#import shutil\n",
    "#shutil.make_archive('./submit/submit-0514-1338-dark/', 'zip', './submit/submit-0514-1338/')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd5a8fc-a1ea-4313-b4c5-4a68bae08aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result\n",
    "#submit-0514-1338, pub score = 0.673835\n",
    "#submit-0514-1338-dark, pub score = 0.674352\n",
    "#divide by 2 but got a higher score, lol."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
